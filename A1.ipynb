{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "For this file, it will use all data to train model.\n",
    "\n",
    "Except MLPClassifier change max_iter to 1"
   ],
   "metadata": {
    "collapsed": false,
    "id": "KVXfExQm8Met"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hqeuq118Mez",
    "outputId": "cb837531-c015-46c5-f1ca-d890f3827658"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from Dataset_Preparation import load_dataset, DATA_PATH\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could adjust param and path of performance.\n",
    "\n",
    "Besides, `DATA_SIZE` is determined the size of dataset you want to use. Set it to 0 means use all data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "PREF_PATH = \"performance.txt\"\n",
    "# set 0 to use all data\n",
    "DATA_SIZE = 0\n",
    "MNB_PARAM = {'alpha': [0.5, 0.0, 1.0, 10.0]}\n",
    "DT_PARAM = {'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [5, 10],\n",
    "            'min_samples_split': [2, 4, 6]}\n",
    "MLP_PARAM = {'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "             'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
    "             'solver': ['sgd', 'adam'],\n",
    "             'max_iter': [1]}"
   ],
   "metadata": {
    "id": "cyghlBpq8Me0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those are functions from `Words_as_features.py`, some of them differ from original ones for compatibility reasons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Process the dataset using feature sklearn.extraction.text.CountVectorizer to extract tokens/words\n",
    "# and their frequencies. Display the number of tokens (the size of the vocabulary) in the dataset\n",
    "def extract_features(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    words = [item[0] for item in data]\n",
    "    count_vector = vectorizer.fit_transform(words)\n",
    "    return vectorizer, count_vector\n",
    "\n",
    "\n",
    "def to_frequencies(X_train_counts):\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf\n",
    "\n",
    "\n",
    "def split_data(data, tokenize=False, tf_idf=False, count_vector=None, size_of_data=DATA_SIZE):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    if tokenize:\n",
    "        X = [word_tokenize(item[0]) for item in data]\n",
    "    else:\n",
    "        if count_vector is not None:\n",
    "            if tf_idf:\n",
    "                X = to_frequencies(count_vector)\n",
    "            else:\n",
    "                X = count_vector\n",
    "        else:\n",
    "            X = [item[0] for item in data]\n",
    "    y1 = [item[1] for item in data]\n",
    "    y2 = [item[2] for item in data]\n",
    "    if size_of_data == 0:\n",
    "        size_of_data = min([len(y1), len(y2)])\n",
    "    return train_test_split(X[:size_of_data], y1[:size_of_data], test_size=0.2, random_state=42), \\\n",
    "           train_test_split(X[:size_of_data], y2[:size_of_data], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def train_models(model, X_train, y_train, param_grid=None):\n",
    "    if param_grid is not None:\n",
    "        model = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def classification_task(model, X_test, y_test):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    y_pred = model.predict(X_test)\n",
    "    matrix = confusion_matrix(y_test, y_pred, labels=np.unique(y_pred))\n",
    "    report = classification_report(y_test, y_pred, labels=np.unique(y_pred))\n",
    "    return matrix, report\n",
    "\n",
    "\n",
    "def save_performance(model, X_test, y_test, data_type: str, path=PREF_PATH):\n",
    "    matrix, report = classification_task(model, X_test, y_test)\n",
    "    result = data_type + \" \" + str(model) + \":\" + str(model.get_params()) + \"\\n\"\n",
    "    result += \"Confusion Matrix: \\n\" + str(matrix) + \"\\n\"\n",
    "    result += \"Classification Report: \\n\" + str(report) + \"\\n\"\n",
    "    result += \"--------------------------------------------\\n\"\n",
    "    print(result)\n",
    "    with open(path, 'a+') as f:\n",
    "        f.write(result)"
   ],
   "metadata": {
    "id": "Tn3R0Y3R8Me1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those are functions from `Embeddings_as_Features.py`, some of them differ from original ones for compatibility reasons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_mean_vector(word2vec_model,  words):\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in word2vec_model.wv.vocab.keys()]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model[words], axis=0)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_post_hit_rate(model, set):\n",
    "    hits = [1 for i in set if i in model.vocab.keys()]\n",
    "    return sum(hits)/len(set)\n",
    "\n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec = []\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if w not in model.vocab.keys():\n",
    "              continue\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw += 1\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "\n",
    "def data_vectorizer(sents, model):\n",
    "    data_vec = []\n",
    "    for sent in sents:\n",
    "        data_vec.append(sent_vectorizer(sent, model))\n",
    "    return data_vec"
   ],
   "metadata": {
    "id": "LMUFyi-A8Me2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is about Words as Features"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fh81kxdw8Me3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def word(tf_idf=False, path=PREF_PATH):\n",
    "\n",
    "    data = load_dataset(DATA_PATH)\n",
    "    vectorizer, count_vector = extract_features(data)\n",
    "    print(\"The size of the vocabulary is: \", len(vectorizer.vocabulary_))\n",
    "    # Split the dataset into 80% for training and 20% for testing\n",
    "\n",
    "    (X_train, X_test, y_train_e, y_test_e), (t1, t2, y_train_s, y_test_s) = split_data(data, tf_idf=tf_idf, count_vector=count_vector)\n",
    "    # X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(X_train[0:100], emotion[0:100], test_size=0.2,\n",
    "    #                                                             random_state=0)\n",
    "    # X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train[0:100], sentiment[0:100], test_size=0.2,\n",
    "    #                                                             random_state=0)\n",
    "\n",
    "    # Train a Multinomial Naive Bayes classifier on the training set\n",
    "\n",
    "    models = [DecisionTreeClassifier(), MultinomialNB(), MLPClassifier(max_iter=1)]\n",
    "    # models = [MLPClassifier(max_iter=1)]\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"\")\n",
    "    for model in models:\n",
    "        emotion_model = train_models(model, X_train, y_train_e)\n",
    "        save_performance(emotion_model, X_test, y_test_e, \"Emotion\",path=path)\n",
    "        sentiment_model = train_models(model, X_train, y_train_s)\n",
    "        save_performance(sentiment_model, X_test, y_test_s, \"Sentiment\",path=path)\n",
    "        print(\"Model trained: \", str(model))\n",
    "        if \"MultinomialNB\" in str(model):\n",
    "            emotion_top_model = train_models(model, X_train, y_train_e, MNB_PARAM)\n",
    "            sentiment_top_model = train_models(model, X_train, y_train_s, MNB_PARAM)\n",
    "        elif \"DecisionTreeClassifier\" in str(model):\n",
    "            emotion_top_model = train_models(model, X_train, y_train_e, DT_PARAM)\n",
    "            sentiment_top_model = train_models(model, X_train, y_train_s, DT_PARAM)\n",
    "        elif \"MLPClassifier\" in str(model):\n",
    "            emotion_top_model = train_models(model, X_train, y_train_e, MLP_PARAM)\n",
    "            sentiment_top_model = train_models(model, X_train, y_train_s, MLP_PARAM)\n",
    "        else:\n",
    "            print(\"Model not found\")\n",
    "            break\n",
    "        save_performance(emotion_top_model, X_test, y_test_e, \"Emotion\", path=path)\n",
    "        save_performance(sentiment_top_model, X_test, y_test_s, \"Sentiment\", path=path)\n",
    "        print(\"GridSearchCV trained: \", str(model))\n",
    "\n",
    "    print(\"Done!\")"
   ],
   "metadata": {
    "id": "yBkwPqCv8Me3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The size of the vocabulary is:  30449\n",
      "Emotion MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[1077   24    4    3   25    9    0    5   10    2    3    1    1    4\n",
      "     0   40   30   74  751   19    0    0    3   36]\n",
      " [  42  688    7    7    2    0    2    4    4    0    2    1    0    1\n",
      "     0    7   25   12  381    4    0    0    2    9]\n",
      " [  12   12  261   31    1    0    1    3    1    1   13   11    0    0\n",
      "     3    4    5    2  633    3    0    2    6    4]\n",
      " [  28   39  107   61    9    8    4    6    2    5   35   15    1    1\n",
      "     5   16    5   12 1268    9    0    7    6   14]\n",
      " [ 152   35   11   12  148   17    6    5    4    2   32    7    0    2\n",
      "     5   19   20   30 1812   26    1    3    5   12]\n",
      " [  22    9    6    8    8   41    0    2    7    2    9    0    0    0\n",
      "     5   13   17   15  496   33    0    6    5    4]\n",
      " [  11   15   12    6    5    0   72   30    1    0   11    1    0    0\n",
      "     4    4    5    6  764    1    1    3    3    9]\n",
      " [  25   11   16    2    6    2    2  139    1    2    4    2    1    4\n",
      "     3    2    1   10  907    7    0    3    2   23]\n",
      " [   8    8    1    0    3    1    0    4   85    0    1    0    0    1\n",
      "     2    2    2    7  256   31    0    0    1    3]\n",
      " [  14    9   21   10    6    6    3    1    8   17   10    7    0    1\n",
      "     3    3    1   11  776    5    1   14   19   11]\n",
      " [  18   24   36   22   28    5    6    2    6    7   69   10    0    1\n",
      "     7   14    5   10 1231   10    1   10   12    8]\n",
      " [   6    3   48   12    3    0    1    1    1    5   10   98    0    0\n",
      "     3    0    2    1  375    2    1    2    5    5]\n",
      " [   4    5   12    6    1    4    3    2    1    1    5    6    2    0\n",
      "     1    0    0    2  229    0    0    6    5    4]\n",
      " [  66    8    6    1    4    2    0    6    4    0    0    0    0   47\n",
      "     0    9   38   12  333   11    1    0    2   22]\n",
      " [   2    8    6    4    4    2    1    1    0    0    2    4    1    0\n",
      "   110    0    1    1  217    3    0    2    1    2]\n",
      " [  90    8    1    0    5    1    0    0    3    0    1    0    0    0\n",
      "     0 1067   29    9  131   21    0   25    3    2]\n",
      " [  58  106    3    1    8    0    0    1    4    1    2    0    0    2\n",
      "     2   37  225   51  329    7    0    0    3   10]\n",
      " [  46   13    4    3    7    1    0    1    1    0    2    1    0    0\n",
      "     0    7   13  655  275    2    0    3    3    0]\n",
      " [ 249  203  138   53   97   31   31   82   50   20   89   22    0   11\n",
      "    28   67   79  135 9411   62   10   31   42   44]\n",
      " [  35   11    6    2   15   11    1    0   18    3    5    1    0    1\n",
      "     1   16   12   13  472  248    0    1    6    6]\n",
      " [  18   16    9    7   13    1    4    2    1    7    9    2    0    1\n",
      "     6    4    5    6  765    9   22   13    4   19]\n",
      " [   2    4    0    0    1    1    0    1    2    0    3    1    1    0\n",
      "     0    4    0    1  125    0    0  152   10    2]\n",
      " [   4    7    8    5    3    5    0    0    6   10    6    3    0    1\n",
      "     3    6    4    5  510    7    1   68  132    3]\n",
      " [  38    9   16    1    1    0    1   10    2    2    2    1    0    3\n",
      "     0    4    7    5  448    3    2    1    1  154]]\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.52      0.51      0.52      2121\n",
      "     amusement       0.54      0.57      0.56      1200\n",
      "         anger       0.35      0.26      0.30      1009\n",
      "     annoyance       0.24      0.04      0.06      1663\n",
      "      approval       0.36      0.06      0.11      2366\n",
      "        caring       0.27      0.06      0.10       708\n",
      "     confusion       0.52      0.07      0.13       964\n",
      "     curiosity       0.45      0.12      0.19      1175\n",
      "        desire       0.38      0.20      0.27       416\n",
      "disappointment       0.19      0.02      0.03       957\n",
      "   disapproval       0.21      0.04      0.07      1542\n",
      "       disgust       0.51      0.17      0.25       584\n",
      " embarrassment       0.29      0.01      0.01       299\n",
      "    excitement       0.58      0.08      0.14       572\n",
      "          fear       0.56      0.30      0.39       372\n",
      "     gratitude       0.78      0.76      0.77      1396\n",
      "           joy       0.41      0.26      0.32       850\n",
      "          love       0.60      0.63      0.62      1037\n",
      "       neutral       0.40      0.86      0.55     10985\n",
      "      optimism       0.47      0.28      0.35       884\n",
      "   realization       0.54      0.02      0.04       943\n",
      "       remorse       0.43      0.49      0.46       310\n",
      "       sadness       0.45      0.17      0.24       797\n",
      "      surprise       0.38      0.22      0.28       711\n",
      "\n",
      "     micro avg       0.44      0.44      0.44     33861\n",
      "     macro avg       0.43      0.26      0.28     33861\n",
      "  weighted avg       0.42      0.44      0.37     33861\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Sentiment MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[ 761  599 1815  618]\n",
      " [ 184 4238 2455  882]\n",
      " [ 448 1783 6616 2138]\n",
      " [ 219  935 2834 7839]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.47      0.20      0.28      3793\n",
      "    negative       0.56      0.55      0.55      7759\n",
      "     neutral       0.48      0.60      0.54     10985\n",
      "    positive       0.68      0.66      0.67     11827\n",
      "\n",
      "    accuracy                           0.57     34364\n",
      "   macro avg       0.55      0.50      0.51     34364\n",
      "weighted avg       0.57      0.57      0.56     34364\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Model trained:  MLPClassifier(max_iter=1)\n",
      "Emotion GridSearchCV(estimator=MLPClassifier(max_iter=1),\n",
      "             param_grid={'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
      "                         'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
      "                         'solver': ['sgd', 'adam']}):{'cv': None, 'error_score': nan, 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__batch_size': 'auto', 'estimator__beta_1': 0.9, 'estimator__beta_2': 0.999, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (100,), 'estimator__learning_rate': 'constant', 'estimator__learning_rate_init': 0.001, 'estimator__max_fun': 15000, 'estimator__max_iter': 1, 'estimator__momentum': 0.9, 'estimator__n_iter_no_change': 10, 'estimator__nesterovs_momentum': True, 'estimator__power_t': 0.5, 'estimator__random_state': None, 'estimator__shuffle': True, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': False, 'estimator__warm_start': False, 'estimator': MLPClassifier(max_iter=1), 'n_jobs': None, 'param_grid': {'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'activation': ['logistic', 'tanh', 'relu', 'identity'], 'solver': ['sgd', 'adam']}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n",
      "Confusion Matrix: \n",
      "[[1089   33    3    2   29    4    0   15   10    0    4    1    1    0\n",
      "    36   40   73  710   26    0    1    6   38]\n",
      " [  43  751   10    2    5    0    2    4    3    0    3    1    1    0\n",
      "     7   29   13  308    6    0    1    0   11]\n",
      " [  11   13  265   17    1    1    2   10    1    2    7   14    0    2\n",
      "     3    6    4  631    4    0    3    7    5]\n",
      " [  22   47  108   34    9    8    3   13    2    2   17   30    0    5\n",
      "    15    6   14 1280   12    0    8   13   15]\n",
      " [ 163   40   10    3  171   20    3    8    5    1   19    6    1    3\n",
      "    16   27   29 1783   31    2    3    8   14]\n",
      " [  23    9    6    3    7   46    1    4    7    1    6    2    0    4\n",
      "    11   21   14  485   39    0    7    9    3]\n",
      " [  13   20    9    3    4    1   54   46    3    0    7    1    1    2\n",
      "     4    4    7  767    2    0    4    3    9]\n",
      " [  26   13   14    1    5    4    2  161    2    0    2    2    2    1\n",
      "     2    3    9  885    9    0    5    7   20]\n",
      " [   8    9    0    2    5    1    0    3   89    0    1    0    1    2\n",
      "     1    3    8  242   38    0    0    1    2]\n",
      " [  17   14   17    6    5    6    3    3    8    5    8    8    1    2\n",
      "     3    4    9  769    5    1   14   41    8]\n",
      " [  20   36   35   12   27    6    6    4    8    2   49   19    0    3\n",
      "    14    5    9 1235   13    1   10   18   10]\n",
      " [   3    4   49   14    4    0    1    1    2    1    6  101    0    4\n",
      "     0    1    2  372    2    0    3    9    5]\n",
      " [  71   10    7    0    5    1    0   10    2    0    0    0   32    0\n",
      "     9   51   14  325   11    0    0    1   23]\n",
      " [   1    8    5    2    2    3    2    1    1    0    3   10    0   75\n",
      "     0    3    1  243    3    0    2    5    2]\n",
      " [  99    8    0    0    7    3    1    0    3    0    0    0    0    0\n",
      "  1054   42    9  114   28    0   23    4    1]\n",
      " [  62  115    4    1    7    0    0    3    4    0    0    0    4    1\n",
      "    22  247   56  300   11    0    0    3   10]\n",
      " [  60   14    3    1    7    0    0    4    0    0    2    0    0    0\n",
      "     8   19  650  257    4    0    1    7    0]\n",
      " [ 277  237  143   27   92   37   21  152   52    6   47   31    4   18\n",
      "    69  102  126 9316   79    5   32   61   51]\n",
      " [  40   14    4    1   12   14    1    3   20    2    6    1    1    2\n",
      "    12   12   13  442  270    0    1    7    6]\n",
      " [  22   18   11    7   14    3    5    2    1    1    9    3    0    5\n",
      "     4    5    7  763   10   10   12    5   26]\n",
      " [   0    5    0    0    1    1    2    1    2    0    2    2    0    0\n",
      "     8    0    1  119    0    0  154   11    1]\n",
      " [   4    9   11    4    3    6    0    0    7    3    6    3    0    2\n",
      "     8    7    4  485    7    0   67  157    4]\n",
      " [  44   13   13    1    2    0    2   17    2    0    1    1    2    1\n",
      "     4   11    5  425    2    1    1    3  160]]\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.50      0.51      0.51      2121\n",
      "     amusement       0.52      0.63      0.57      1200\n",
      "         anger       0.35      0.26      0.30      1009\n",
      "     annoyance       0.23      0.02      0.04      1663\n",
      "      approval       0.40      0.07      0.12      2366\n",
      "        caring       0.27      0.06      0.10       708\n",
      "     confusion       0.47      0.06      0.10       964\n",
      "     curiosity       0.34      0.14      0.20      1175\n",
      "        desire       0.38      0.21      0.27       416\n",
      "disappointment       0.15      0.01      0.01       957\n",
      "   disapproval       0.23      0.03      0.06      1542\n",
      "       disgust       0.40      0.17      0.24       584\n",
      "    excitement       0.63      0.06      0.10       572\n",
      "          fear       0.52      0.20      0.29       372\n",
      "     gratitude       0.80      0.76      0.78      1396\n",
      "           joy       0.37      0.29      0.33       850\n",
      "          love       0.60      0.63      0.61      1037\n",
      "       neutral       0.41      0.85      0.55     10985\n",
      "      optimism       0.44      0.31      0.36       884\n",
      "   realization       0.50      0.01      0.02       943\n",
      "       remorse       0.43      0.50      0.46       310\n",
      "       sadness       0.39      0.20      0.26       797\n",
      "      surprise       0.37      0.23      0.28       711\n",
      "\n",
      "     micro avg       0.43      0.45      0.44     33562\n",
      "     macro avg       0.42      0.27      0.29     33562\n",
      "  weighted avg       0.42      0.45      0.37     33562\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Sentiment GridSearchCV(estimator=MLPClassifier(max_iter=1),\n",
      "             param_grid={'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
      "                         'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
      "                         'solver': ['sgd', 'adam']}):{'cv': None, 'error_score': nan, 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__batch_size': 'auto', 'estimator__beta_1': 0.9, 'estimator__beta_2': 0.999, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (100,), 'estimator__learning_rate': 'constant', 'estimator__learning_rate_init': 0.001, 'estimator__max_fun': 15000, 'estimator__max_iter': 1, 'estimator__momentum': 0.9, 'estimator__n_iter_no_change': 10, 'estimator__nesterovs_momentum': True, 'estimator__power_t': 0.5, 'estimator__random_state': None, 'estimator__shuffle': True, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': False, 'estimator__warm_start': False, 'estimator': MLPClassifier(max_iter=1), 'n_jobs': None, 'param_grid': {'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'activation': ['logistic', 'tanh', 'relu', 'identity'], 'solver': ['sgd', 'adam']}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n",
      "Confusion Matrix: \n",
      "[[ 782  585 1846  580]\n",
      " [ 205 4178 2541  835]\n",
      " [ 474 1724 6712 2075]\n",
      " [ 243  936 2889 7759]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.46      0.21      0.28      3793\n",
      "    negative       0.56      0.54      0.55      7759\n",
      "     neutral       0.48      0.61      0.54     10985\n",
      "    positive       0.69      0.66      0.67     11827\n",
      "\n",
      "    accuracy                           0.57     34364\n",
      "   macro avg       0.55      0.50      0.51     34364\n",
      "weighted avg       0.57      0.57      0.56     34364\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "GridSearchCV trained:  MLPClassifier(max_iter=1)\n",
      "Done!\n",
      "The size of the vocabulary is:  30449\n",
      "Emotion MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[ 1009    22     2     0     1     1     0     4     3     0     1     0\n",
      "      0     0    40    18    85   916    14     0     0     0     5]\n",
      " [   38   614     1     0     0     0     0     2     1     0     0     0\n",
      "      0     0     5    17     9   508     2     0     0     0     3]\n",
      " [   12     8   171    12     0     0     0    10     0     0     7     3\n",
      "      0     0     3     3     5   766     3     0     0     4     2]\n",
      " [   18    25    47    19     2     0     0    11     1     0    14     3\n",
      "      1     0    12     2    12  1482     4     0     3     3     4]\n",
      " [  130    28     2     1    66     2     0     5     1     0    12     0\n",
      "      1     0    18    13    28  2034    16     0     1     4     4]\n",
      " [   21     6     2     0     0    13     0     2     1     0     2     0\n",
      "      0     0    14    14    15   569    37     0     4     6     2]\n",
      " [   10    21     7     0     0     0    18    26     0     0     5     0\n",
      "      0     0     3     4     3   861     0     0     2     2     2]\n",
      " [   17     9    11     1     0     0     1   118     1     0     1     0\n",
      "      0     0     2     1     6   999     3     0     3     1     1]\n",
      " [   11     8     0     0     0     0     0     3    41     0     0     0\n",
      "      0     1     2     1     8   312    26     0     0     2     1]\n",
      " [    9     9     9     3     1     2     2     5     2     1     4     1\n",
      "      0     0     2     2    11   861     4     0     4    19     6]\n",
      " [   15    21    15     1     9     0     3     2     0     0    42     2\n",
      "      0     0    12     4     7  1390     9     0     4     4     2]\n",
      " [    9     1    21     3     0     0     0     0     1     0     6    35\n",
      "      0     1     0     0     0   499     2     0     0     4     2]\n",
      " [   65     3     2     0     1     0     0     1     1     0     0     0\n",
      "     11     0    10    33    11   422     9     0     0     0     3]\n",
      " [    6     9     3     0     2     0     0     2     0     0     1     1\n",
      "      0    19     0     0     1   321     3     0     0     2     2]\n",
      " [   83     9     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0  1039    31    10   195    15     0     8     6     0]\n",
      " [   53    88     2     0     0     0     0     0     1     0     0     0\n",
      "      1     1    28   168    60   441     5     0     0     1     1]\n",
      " [   42     9     2     0     2     0     0     3     0     0     0     0\n",
      "      0     0     8     9   644   312     2     0     2     2     0]\n",
      " [  206   166    51     4    24     5     2    76    18     0    31     1\n",
      "      1     1    62    55   115 10088    40     0     9    23     7]\n",
      " [   33     7     2     0     2     0     0     0     6     0     1     0\n",
      "      1     0    15     3    13   609   188     0     0     3     1]\n",
      " [   16    11     3     0     1     0     1     1     0     0     4     1\n",
      "      0     0     2     3     5   880     4     2     3     4     2]\n",
      " [    1     5     1     0     1     0     0     0     2     0     1     0\n",
      "      0     0     4     0     1   201     0     0    57    36     0]\n",
      " [    6    12     5     1     0     1     0     0     2     0     3     1\n",
      "      0     1     3     4     7   613     5     0    25   108     0]\n",
      " [   45     6    11     0     0     1     0    15     1     0     1     0\n",
      "      1     0     5     7     4   554     1     0     0     2    57]]\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.53      0.48      0.50      2121\n",
      "     amusement       0.56      0.51      0.53      1200\n",
      "         anger       0.45      0.17      0.25      1009\n",
      "     annoyance       0.40      0.01      0.02      1663\n",
      "      approval       0.58      0.03      0.05      2366\n",
      "        caring       0.50      0.02      0.04       708\n",
      "     confusion       0.64      0.02      0.04       964\n",
      "     curiosity       0.41      0.10      0.16      1175\n",
      "        desire       0.49      0.10      0.16       416\n",
      "disappointment       1.00      0.00      0.00       957\n",
      "   disapproval       0.30      0.03      0.05      1542\n",
      "       disgust       0.73      0.06      0.11       584\n",
      "    excitement       0.65      0.02      0.04       572\n",
      "          fear       0.73      0.05      0.10       372\n",
      "     gratitude       0.80      0.74      0.77      1396\n",
      "           joy       0.41      0.20      0.27       850\n",
      "          love       0.60      0.62      0.61      1037\n",
      "       neutral       0.38      0.92      0.54     10985\n",
      "      optimism       0.48      0.21      0.29       884\n",
      "   realization       1.00      0.00      0.00       943\n",
      "       remorse       0.43      0.18      0.26       310\n",
      "       sadness       0.45      0.14      0.21       797\n",
      "      surprise       0.53      0.08      0.14       711\n",
      "\n",
      "     micro avg       0.42      0.43      0.43     33562\n",
      "     macro avg       0.57      0.20      0.22     33562\n",
      "  weighted avg       0.51      0.43      0.33     33562\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Sentiment MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[ 774  562 1821  636]\n",
      " [ 200 4017 2634  908]\n",
      " [ 468 1618 6742 2157]\n",
      " [ 227  834 2950 7816]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.46      0.20      0.28      3793\n",
      "    negative       0.57      0.52      0.54      7759\n",
      "     neutral       0.48      0.61      0.54     10985\n",
      "    positive       0.68      0.66      0.67     11827\n",
      "\n",
      "    accuracy                           0.56     34364\n",
      "   macro avg       0.55      0.50      0.51     34364\n",
      "weighted avg       0.57      0.56      0.56     34364\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Model trained:  MLPClassifier(max_iter=1)\n",
      "Emotion GridSearchCV(estimator=MLPClassifier(max_iter=1),\n",
      "             param_grid={'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
      "                         'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
      "                         'solver': ['sgd', 'adam']}):{'cv': None, 'error_score': nan, 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__batch_size': 'auto', 'estimator__beta_1': 0.9, 'estimator__beta_2': 0.999, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (100,), 'estimator__learning_rate': 'constant', 'estimator__learning_rate_init': 0.001, 'estimator__max_fun': 15000, 'estimator__max_iter': 1, 'estimator__momentum': 0.9, 'estimator__n_iter_no_change': 10, 'estimator__nesterovs_momentum': True, 'estimator__power_t': 0.5, 'estimator__random_state': None, 'estimator__shuffle': True, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': False, 'estimator__warm_start': False, 'estimator': MLPClassifier(max_iter=1), 'n_jobs': None, 'param_grid': {'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'activation': ['logistic', 'tanh', 'relu', 'identity'], 'solver': ['sgd', 'adam']}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n",
      "Confusion Matrix: \n",
      "[[1091   25    2    0    0    3    0    5    1    0    1    0    0   35\n",
      "    19   68  851   10    0    0   10]\n",
      " [  58  663    4    0    0    0    0    2    1    2    1    0    0    6\n",
      "    11    6  441    1    0    1    3]\n",
      " [  13   12  251    2    0    0    0   11    0    5    5    0    0    4\n",
      "     1    3  688    3    0   10    1]\n",
      " [  22   37   98    4    0    1    0   11    0   16   13    0    0   11\n",
      "     2    9 1418    2    4   12    3]\n",
      " [ 153   32    8    0   13    3    0    2    0   15    3    0    0   16\n",
      "    11   28 2060   12    0    6    4]\n",
      " [  30    8    6    0    0   22    0    0    1    2    0    0    0   15\n",
      "     7   15  562   24    4   11    1]\n",
      " [  10   22   12    0    0    0    2   25    0   13    1    0    0    3\n",
      "     2    4  861    0    2    5    2]\n",
      " [  26   11   22    0    0    0    0  104    0    0    2    0    0    2\n",
      "     0    5  992    4    3    2    2]\n",
      " [  13    9    4    0    0    1    0    2   20    0    0    0    0    2\n",
      "     2    7  314   27    0   13    2]\n",
      " [  14   28   28    2    6    0    0    1    0   40   10    0    0   12\n",
      "     2    9 1356    8    2   22    2]\n",
      " [   8    3   44    2    0    0    0    3    0    9   60    0    0    0\n",
      "     0    1  438    2    0   13    1]\n",
      " [ 102    9    4    0    0    0    0    3    1    0    0    1    0    6\n",
      "    30   10  392    5    0    0    9]\n",
      " [   3    8   10    0    0    2    0    0    0    1    7    0    8    0\n",
      "     2    3  290    9    0   27    2]\n",
      " [ 103    9    0    0    0    4    0    0    1    0    0    0    0 1027\n",
      "    28   10  180   12    7   15    0]\n",
      " [  99  100    3    0    0    2    0    0    1    0    0    0    0   25\n",
      "   134   45  428    7    0    3    3]\n",
      " [  91    8    2    0    0    1    0    0    0    0    1    0    0    9\n",
      "     7  591  319    1    1    6    0]\n",
      " [ 309  176  107    2    3   17    0   76    3   43   11    0    0   57\n",
      "    40   99 9917   36    8   69   12]\n",
      " [  41    9    5    0    0   11    0    1    1    2    0    0    0   13\n",
      "     2   13  622  151    0   12    1]\n",
      " [   0    7    0    0    0    2    0    1    0    2    2    0    0    8\n",
      "     0    1  148    0   52   87    0]\n",
      " [   4    9    6    1    0    4    0    0    2    9    3    0    0    8\n",
      "     3    5  539    3   24  177    0]\n",
      " [  57   15   15    0    0    1    0   19    0    2    0    0    0    7\n",
      "     5    2  517    0    1    5   65]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  admiration       0.47      0.51      0.49      2121\n",
      "   amusement       0.53      0.55      0.54      1200\n",
      "       anger       0.37      0.25      0.30      1009\n",
      "   annoyance       0.27      0.00      0.00      1663\n",
      "    approval       0.59      0.01      0.01      2366\n",
      "      caring       0.28      0.03      0.06       708\n",
      "   confusion       1.00      0.00      0.00       964\n",
      "   curiosity       0.38      0.09      0.14      1175\n",
      "      desire       0.61      0.05      0.09       416\n",
      " disapproval       0.22      0.03      0.05      1542\n",
      "     disgust       0.44      0.10      0.17       584\n",
      "  excitement       1.00      0.00      0.00       572\n",
      "        fear       1.00      0.02      0.04       372\n",
      "   gratitude       0.80      0.74      0.76      1396\n",
      "         joy       0.41      0.16      0.23       850\n",
      "        love       0.62      0.57      0.59      1037\n",
      "     neutral       0.39      0.90      0.54     10985\n",
      "    optimism       0.46      0.17      0.25       884\n",
      "     remorse       0.43      0.17      0.24       310\n",
      "     sadness       0.30      0.22      0.26       797\n",
      "    surprise       0.49      0.09      0.15       711\n",
      "\n",
      "   micro avg       0.42      0.45      0.44     31662\n",
      "   macro avg       0.53      0.22      0.23     31662\n",
      "weighted avg       0.47      0.45      0.34     31662\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Sentiment GridSearchCV(estimator=MLPClassifier(max_iter=1),\n",
      "             param_grid={'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
      "                         'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
      "                         'solver': ['sgd', 'adam']}):{'cv': None, 'error_score': nan, 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__batch_size': 'auto', 'estimator__beta_1': 0.9, 'estimator__beta_2': 0.999, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (100,), 'estimator__learning_rate': 'constant', 'estimator__learning_rate_init': 0.001, 'estimator__max_fun': 15000, 'estimator__max_iter': 1, 'estimator__momentum': 0.9, 'estimator__n_iter_no_change': 10, 'estimator__nesterovs_momentum': True, 'estimator__power_t': 0.5, 'estimator__random_state': None, 'estimator__shuffle': True, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': False, 'estimator__warm_start': False, 'estimator': MLPClassifier(max_iter=1), 'n_jobs': None, 'param_grid': {'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'activation': ['logistic', 'tanh', 'relu', 'identity'], 'solver': ['sgd', 'adam']}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n",
      "Confusion Matrix: \n",
      "[[ 760  574 1689  770]\n",
      " [ 165 4010 2476 1108]\n",
      " [ 430 1630 6332 2593]\n",
      " [ 198  792 2566 8271]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.49      0.20      0.28      3793\n",
      "    negative       0.57      0.52      0.54      7759\n",
      "     neutral       0.48      0.58      0.53     10985\n",
      "    positive       0.65      0.70      0.67     11827\n",
      "\n",
      "    accuracy                           0.56     34364\n",
      "   macro avg       0.55      0.50      0.51     34364\n",
      "weighted avg       0.56      0.56      0.55     34364\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "GridSearchCV trained:  MLPClassifier(max_iter=1)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "word(path=PREF_PATH)\n",
    "# Use tf-idf instead of word frequencies and redo all substeps of 2.3 above ‚Äì you can use TfidfTransformer\n",
    "# for this. Display the results of this experiment\n",
    "\n",
    "word(tf_idf=True,path = \"tf_performance.txt\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSt_4CS_8Me4",
    "outputId": "17a0eae4-7b67-491f-a537-cf0d9e93e430"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def embedding(path=PREF_PATH,size=100,vector=None):\n",
    "    google_vectors = vector\n",
    "    print(google_vectors.vector_size)\n",
    "    datas = load_dataset(DATA_PATH)\n",
    "    # clean data\n",
    "    data = []\n",
    "    for value in datas:\n",
    "      words = word_tokenize(value[0])\n",
    "      if len(get_mean_vector(google_vectors, words)) >= 1:\n",
    "        data.append(value)\n",
    "    print(\"total remove:\" + str(len(datas) - len(data)))\n",
    "    # 3.2\n",
    "    (X_train, X_test, y_train_e, y_test_e), (_, _, y_train_s, y_test_s) = split_data(data, tokenize=True, size_of_data=size)\n",
    "\n",
    "    print(\"The size of the tokens is: \", len(X_train))\n",
    "    # Compute the embedding of a Reddit post as the average of the embeddings of its words. If\n",
    "    # a word has no embedding in Word2Vec, skip it.\n",
    "    sentence_embeddings = get_mean_vector(google_vectors, X_train[0])\n",
    "    average = sum(sentence_embeddings)/len(sentence_embeddings)\n",
    "    print(average)\n",
    "    # 3.3\n",
    "    embeddings_train = []\n",
    "    for post in X_train:\n",
    "        embedding = get_mean_vector(google_vectors, post)\n",
    "        embeddings_train.append(sum(embedding)/len(embedding)if len(embedding)!=0 else 0)\n",
    "\n",
    "    print(embeddings_train[0:100])\n",
    "\n",
    "\n",
    "    # 3.4\n",
    "    # Compute and display the overall hit rates of the training and test sets\n",
    "    train_hit_rates = []\n",
    "    for post in X_train[0:100]:\n",
    "        train_hit_rates.append(get_post_hit_rate(google_vectors, post))\n",
    "    print(\"The hit rate of the training set is: \", sum(train_hit_rates)/len(train_hit_rates))\n",
    "    test_hit_rates = []\n",
    "    for post in X_test[0:100]:\n",
    "        test_hit_rates.append(get_post_hit_rate(google_vectors, post))\n",
    "    print(\"The hit rate of the test set is: \", sum(test_hit_rates)/len(test_hit_rates))\n",
    "\n",
    "    # 3.5\n",
    "\n",
    "    X_train = data_vectorizer(X_train, google_vectors)\n",
    "    X_test = data_vectorizer(X_test, google_vectors)\n",
    "    # X_train = embeddings_train\n",
    "    # X_test = embeddings_test\n",
    "    # test = 0\n",
    "    # for value in X_train:\n",
    "    #   if test == 0:\n",
    "    #     test = value.shape\n",
    "    #   elif value.shape != test:\n",
    "    #     print(value.shape)\n",
    "    #     break\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(\"\")\n",
    "    model = MLPClassifier(max_iter=1)\n",
    "    base_MLP_e = train_models(model, X_train, y_train_e)\n",
    "    save_performance(base_MLP_e, X_test, y_test_e, \"base_MLP_e\", path=path)\n",
    "    base_MLP_s = train_models(model, X_train, y_train_s)\n",
    "    save_performance(base_MLP_s, X_test, y_test_s, \"base_MLP_s\", path=path)\n",
    "    top_MLP_e = train_models(model, X_train, y_train_e, param_grid=MLP_PARAM)\n",
    "    save_performance(top_MLP_e, X_test, y_test_e, \"top_MLP_e\", path=path)\n",
    "    top_MLP_s = train_models(model, X_train, y_train_s, param_grid=MLP_PARAM)\n",
    "    save_performance(top_MLP_s, X_test, y_test_s, \"top_MLP_s\", path=path)\n",
    "    # print(type(base_MLP_e), type(top_MLP_e), type(base_MLP_s), type(top_MLP_s))\n",
    "    "
   ],
   "metadata": {
    "id": "8MDOAjCp8Me4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "google = api.load(\"word2vec-google-news-300\")"
   ],
   "metadata": {
    "id": "hwOp-fC2d-KJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n",
      "['RemindMe', '!']\n",
      "removed:RemindMe!\n",
      "['and', '...', '?']\n",
      "removed:and...?\n",
      "['cuteee']\n",
      "removed:cuteee\n",
      "['...', 'and', '?']\n",
      "removed:...and?\n",
      "['judah', '!', '!']\n",
      "removed:judah!!\n",
      "['(', ':', ')']\n",
      "removed:(:)\n",
      "['A-fucking-men', '!', ':', ')']\n",
      "removed:A-fucking-men! :)\n",
      "['...', 'and', '?']\n",
      "removed:...and?\n",
      "['Intp', ':', '(']\n",
      "removed:Intp:(\n",
      "['Ahahahahahaha', '!', '!']\n",
      "removed:Ahahahahahaha!!\n",
      "['Infacy', 'gosples', '?']\n",
      "removed:Infacy gosples?\n",
      "['and', '...', '?']\n",
      "removed:and...?\n",
      "['Nope.cool', '!', 'üÖ±Ô∏èRO']\n",
      "removed:Nope.cool !üÖ±Ô∏èRO\n",
      "['Ahahahahahaha', '!', '!']\n",
      "removed:Ahahahahahaha!!\n",
      "['^She', '^took', '^me', '^by', '^the', '^hand', '...', '^made', '^me', '^a', '^man', '...', '^THAT', '^ONE', '^NIGHT', '!']\n",
      "removed:^She ^took ^me ^by ^the ^hand... ^made ^me ^a ^man... ^THAT ^ONE ^NIGHT!\n",
      "['Infacy', 'gosples', '?']\n",
      "removed:Infacy gosples?\n",
      "['Chuckled', '.', 'Upvote', '!']\n",
      "removed:Chuckled. Upvote!\n",
      "['DUTCHY', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!']\n",
      "removed:DUTCHY!!!!!!!!!!!!!\n",
      "['whyyyyy', '!', '!', '!', '!', 'ughhh']\n",
      "removed:whyyyyy !!!! ughhh\n",
      "['whyyyyy', '!', '!', '!', '!', 'ughhh']\n",
      "removed:whyyyyy !!!! ughhh\n",
      "['Yaaaay', '!', '!']\n",
      "removed:Yaaaay!!\n",
      "[':', '(', '(']\n",
      "removed::((\n",
      "['SONNY-TATER', '!', '!']\n",
      "removed:SONNY-TATER!!\n",
      "['GYROBALL', '!', '!', '!']\n",
      "removed:GYROBALL!!!\n",
      "['Chuckled', '.', 'Upvote', '!']\n",
      "removed:Chuckled. Upvote!\n",
      "['CURRRRRRRRY', '!']\n",
      "removed:CURRRRRRRRY!\n",
      "['Smoooootthhhhhhh', '!', '!']\n",
      "removed:Smoooootthhhhhhh!!\n",
      "['GYROBALL', '!', '!', '!']\n",
      "removed:GYROBALL!!!\n",
      "['r/LazyMan', '.', '1080p', '60fps', '.']\n",
      "removed:r/LazyMan. 1080p 60fps.\n",
      "['r/LazyMan', '.', '1080p', '60fps', '.']\n",
      "removed:r/LazyMan. 1080p 60fps.\n",
      "['judah', '!', '!']\n",
      "removed:judah!!\n",
      "['SONNY-TATER', '!', '!']\n",
      "removed:SONNY-TATER!!\n",
      "['RemindMe', '!']\n",
      "removed:RemindMe!\n",
      "['SWOOOSH', '!', '!']\n",
      "removed:SWOOOSH!!\n",
      "['!', 'Messageme', 'creepypost']\n",
      "removed:!Messageme creepypost\n",
      "['Intp', ':', '(']\n",
      "removed:Intp:(\n",
      "['!', 'Messageme', 'creepypost']\n",
      "removed:!Messageme creepypost\n",
      "['...', 'and', '?']\n",
      "removed:...and?\n",
      "['Yaaaay', '!', '!']\n",
      "removed:Yaaaay!!\n",
      "['FoX', 'cHaNnEl', 'BaD', '!', '!', '!', '!', '!']\n",
      "removed:FoX cHaNnEl BaD!!!!!\n",
      "['r/NotHowLesbiansWork']\n",
      "removed:r/NotHowLesbiansWork\n",
      "['RemindMe', '!', '12hours']\n",
      "removed:RemindMe! 12hours\n",
      "['!', 'Messageme', 'creepypost']\n",
      "removed:!Messageme creepypost\n",
      "['DUTCHY', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!']\n",
      "removed:DUTCHY!!!!!!!!!!!!!\n",
      "['A-fucking-men', '!', ':', ')']\n",
      "removed:A-fucking-men! :)\n",
      "['CURRRRRRRRY', '!']\n",
      "removed:CURRRRRRRRY!\n",
      "['SWOOOSH', '!', '!']\n",
      "removed:SWOOOSH!!\n",
      "['^She', '^took', '^me', '^by', '^the', '^hand', '...', '^made', '^me', '^a', '^man', '...', '^THAT', '^ONE', '^NIGHT', '!']\n",
      "removed:^She ^took ^me ^by ^the ^hand... ^made ^me ^a ^man... ^THAT ^ONE ^NIGHT!\n",
      "['r/NotHowLesbiansWork']\n",
      "removed:r/NotHowLesbiansWork\n",
      "['and', '...', '?']\n",
      "removed:and...?\n",
      "['Chuckled', '.', 'Upvote', '!']\n",
      "removed:Chuckled. Upvote!\n",
      "['Nope.cool', '!', 'üÖ±Ô∏èRO']\n",
      "removed:Nope.cool !üÖ±Ô∏èRO\n",
      "['üòò', '‚òÇ', '‚òÇÔ∏è‚òÇÔ∏è']\n",
      "removed:üòò ‚òÇ ‚òÇÔ∏è‚òÇÔ∏è\n",
      "['^She', '^took', '^me', '^by', '^the', '^hand', '...', '^made', '^me', '^a', '^man', '...', '^THAT', '^ONE', '^NIGHT', '!']\n",
      "removed:^She ^took ^me ^by ^the ^hand... ^made ^me ^a ^man... ^THAT ^ONE ^NIGHT!\n",
      "['Congratulatioins', '!']\n",
      "removed:Congratulatioins!\n",
      "['cuteee']\n",
      "removed:cuteee\n",
      "['cuteee']\n",
      "removed:cuteee\n",
      "['whyyyyy', '!', '!', '!', '!', 'ughhh']\n",
      "removed:whyyyyy !!!! ughhh\n",
      "['Yaaaay', '!', '!']\n",
      "removed:Yaaaay!!\n",
      "['CURRRRRRRRY', '!']\n",
      "removed:CURRRRRRRRY!\n",
      "[':', '(', '(']\n",
      "removed::((\n",
      "['A-fucking-men', '!', ':', ')']\n",
      "removed:A-fucking-men! :)\n",
      "['Smoooootthhhhhhh', '!', '!']\n",
      "removed:Smoooootthhhhhhh!!\n",
      "['RemindMe', '!']\n",
      "removed:RemindMe!\n",
      "['Chuckled', '.', 'Upvote', '!']\n",
      "removed:Chuckled. Upvote!\n",
      "['FoX', 'cHaNnEl', 'BaD', '!', '!', '!', '!', '!']\n",
      "removed:FoX cHaNnEl BaD!!!!!\n",
      "['and', '...', '?']\n",
      "removed:and...?\n",
      "['DUTCHY', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!']\n",
      "removed:DUTCHY!!!!!!!!!!!!!\n",
      "['(', ':', ')']\n",
      "removed:(:)\n",
      "['SWOOOSH', '!', '!']\n",
      "removed:SWOOOSH!!\n",
      "['Nope.cool', '!', 'üÖ±Ô∏èRO']\n",
      "removed:Nope.cool !üÖ±Ô∏èRO\n",
      "['Chuckled', '.', 'Upvote', '!']\n",
      "removed:Chuckled. Upvote!\n",
      "['ü¶ÄMY', 'BABYS', 'DEADü¶Ä']\n",
      "removed:ü¶ÄMY BABYS DEADü¶Ä\n",
      "['r/LazyMan', '.', '1080p', '60fps', '.']\n",
      "removed:r/LazyMan. 1080p 60fps.\n",
      "['Congratulatioins', '!']\n",
      "removed:Congratulatioins!\n",
      "['FoX', 'cHaNnEl', 'BaD', '!', '!', '!', '!', '!']\n",
      "removed:FoX cHaNnEl BaD!!!!!\n",
      "['RemindMe', '!', '12hours']\n",
      "removed:RemindMe! 12hours\n",
      "['Ahahahahahaha', '!', '!']\n",
      "removed:Ahahahahahaha!!\n",
      "['FoX', 'cHaNnEl', 'BaD', '!', '!', '!', '!', '!']\n",
      "removed:FoX cHaNnEl BaD!!!!!\n",
      "['Smoooootthhhhhhh', '!', '!']\n",
      "removed:Smoooootthhhhhhh!!\n",
      "['SONNY-TATER', '!', '!']\n",
      "removed:SONNY-TATER!!\n",
      "['ü¶ÄMY', 'BABYS', 'DEADü¶Ä']\n",
      "removed:ü¶ÄMY BABYS DEADü¶Ä\n",
      "['GYROBALL', '!', '!', '!']\n",
      "removed:GYROBALL!!!\n",
      "['Infacy', 'gosples', '?']\n",
      "removed:Infacy gosples?\n",
      "['RemindMe', '!', '12hours']\n",
      "removed:RemindMe! 12hours\n",
      "['BleachedAssholePink', '.', 'üòÇ']\n",
      "removed:BleachedAssholePink. üòÇ\n",
      "['A-fucking-men', '!', ':', ')']\n",
      "removed:A-fucking-men! :)\n",
      "The size of the tokens is:  137386\n",
      "0.0022686354534622905\n",
      "[0.0022686354534622905, 0.001817935375244512, -4.071177128935233e-05, -0.0012224695291661192, -0.0013817632226952507, -0.0013735536541450225, -0.004320715065308226, 0.005975932487053796, -0.006749481179867871, 0.0013001209649761828, -0.0026429810375945334, -0.0035659609322647155, 0.004770722600381608, 0.000793251246213913, -0.001133957713603498, -0.006623834815654845, -0.0025031900847291883, -0.00922635613727228, -0.0020347289457761995, -0.002057491632876918, -0.003030980475305114, -0.0012248413877872129, 0.0035101476210790375, -0.002130294069647789, -0.0017236201224053124, -0.0038226531442584625, -0.000817759536827604, -0.002248188662342727, -0.001216347146643481, -0.0021061888337135315, -0.0034706993108557073, -0.0027044802800325366, -0.014583488003506015, 0.0008476275205612183, -0.0006662869453430176, -0.001958352236348825, -0.003818589262276267, 0.0013712600397411735, -0.00446600618888624, -0.0016871091863625528, -0.00391344522048712, -0.00047506341764043706, 0.00043071318621514367, 0.0016088859977511068, 0.0018899674603017047, -0.001611193943535909, -0.0025743420654907824, -0.001997985043951, -0.0010970518554677254, -0.002049878714933584, -0.00037593638660230984, 0.0016462314128875733, 0.0005896774206045545, -0.0028004987475287635, -0.0007915883979876526, -0.0032757113542659986, -0.0020244624992240765, -5.4773451896229135e-05, -0.001600753551950523, -0.005736716588338216, -0.0017151234997254505, -0.003937298843690466, -0.0005985055863857269, 0.001353781639918452, 0.0029326218366622927, -0.0055510617059189825, -0.00754731280496344, 0.0025177951279329136, -0.0019652673518673206, -0.0044824009352790505, -0.005717008925275877, -0.0030753784449916565, -0.0015275244059739634, -0.004425337165594101, 0.004739723205566406, -0.0001735511927351278, -0.008256288468837738, 0.0017540970985040379, -0.0014448102315266927, -0.004101745287577311, 0.00031289909461823604, -0.003387580911318461, 0.00039734273702682305, -0.0035050332314373613, -0.002702837387720744, -0.0007673694573653241, 0.0022270886103312173, 0.0022235325311582223, -0.0020315686806376713, 0.0022771291166282025, -0.004971675992079933, -0.004054238006162147, 0.0008247962273890152, 0.00043897678951907435, -0.002641661692565928, 0.0005965557822491974, -0.002936131339520216, 0.0016088859977511068, -0.0008913261224370217, 0.0003310221401382781]\n",
      "The hit rate of the training set is:  0.7748245305199692\n",
      "The hit rate of the test set is:  0.7652190467686953\n",
      "base_MLP_e MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[  654    26     4     0     0     0     0     4     0     1     1     1\n",
      "      0    78     4    65  1265     3     0     0     0]\n",
      " [   38   230     4     0     0     0     0     0     0     3     0     0\n",
      "      0     9     5     8   911     1     0     1     0]\n",
      " [    8    15    91     7     0     0     0     6     0     4     5     0\n",
      "      0     4     0    10   891     1     0     3     0]\n",
      " [    7    22    39     8     0     0     0     2     1     8     9     0\n",
      "      0     7     1     7  1537     1     0     2     0]\n",
      " [   79    24     3     1     9     1     0     1     0     3     1     0\n",
      "      0    23     0    35  2070     1     0     0     1]\n",
      " [   15     2     1     0     0    11     0     0     0     4     0     0\n",
      "      0    29     5     4   595     5     1     1     0]\n",
      " [    4     7     5     0     0     0     0     9     0     6     2     0\n",
      "      0     3     0     5   951     0     2     1     0]\n",
      " [    8     4     2     0     0     0     1    33     0     1     2     0\n",
      "      0     5     0     7  1116     2     1     0     0]\n",
      " [   10     7     2     1     0     0     0     0     2     4     5     0\n",
      "      0     1     1     7   922     0     0     6     0]\n",
      " [   12    16     7     2     0     0     0     1     0    26     5     0\n",
      "      0     7     1     7  1387     0     0     1     0]\n",
      " [    6     4    20     1     0     0     0     0     1     2    31     0\n",
      "      0     0     0     4   535     0     0     1     0]\n",
      " [   47    10     3     0     0     0     0     2     0     0     1     2\n",
      "      0    19    33    11   471     0     0     0     1]\n",
      " [    4     3     2     0     0     0     0     1     0     0    23     0\n",
      "      0     0     0     0   310     0     0     4     0]\n",
      " [   36     5     0     0     0     0     0     1     0     1     0     0\n",
      "      0  1026     9    10   389     2     3     1     0]\n",
      " [   60    36     0     0     0     0     0     1     0     1     1     2\n",
      "      0    37    45    45   630     2     0     0     0]\n",
      " [   42    12     0     0     0     1     0     0     0     0     0     0\n",
      "      0    12     3   453   458     0     0     0     0]\n",
      " [  204    78    64     7     6     6     0    47     2    23    19     0\n",
      "      1    87    22    86 10351    16     4    11     0]\n",
      " [   33     0     0     0     1     0     0     0     0     1     0     0\n",
      "      0    36     6    14   779    34     0     0     0]\n",
      " [    0     3     0     1     0     0     0     0     0     4     0     0\n",
      "      0    11     0     0   268     0    17     2     0]\n",
      " [    6     9     4     0     0     0     0     1     1     4     7     0\n",
      "      0     2     1    12   643     4    11    30     0]\n",
      " [   22    17     7     0     0     0     0     5     0     1     5     0\n",
      "      0     2     3     1   644     0     0     0     1]]\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.49      0.31      0.38      2106\n",
      "     amusement       0.42      0.19      0.26      1210\n",
      "         anger       0.34      0.09      0.14      1045\n",
      "     annoyance       0.25      0.00      0.01      1651\n",
      "      approval       0.56      0.00      0.01      2252\n",
      "        caring       0.58      0.02      0.03       673\n",
      "     confusion       0.00      0.00      0.00       995\n",
      "     curiosity       0.29      0.03      0.05      1182\n",
      "disappointment       0.25      0.00      0.00       968\n",
      "   disapproval       0.25      0.02      0.03      1472\n",
      "       disgust       0.25      0.05      0.09       605\n",
      "    excitement       0.40      0.00      0.01       600\n",
      "          fear       0.00      0.00      0.00       347\n",
      "     gratitude       0.71      0.69      0.70      1483\n",
      "           joy       0.31      0.05      0.09       860\n",
      "          love       0.55      0.46      0.50       981\n",
      "       neutral       0.35      0.94      0.52     11034\n",
      "      optimism       0.43      0.04      0.07       904\n",
      "       remorse       0.41      0.06      0.10       306\n",
      "       sadness       0.38      0.04      0.07       735\n",
      "      surprise       0.33      0.00      0.00       708\n",
      "\n",
      "     micro avg       0.38      0.41      0.39     32117\n",
      "     macro avg       0.36      0.14      0.15     32117\n",
      "  weighted avg       0.38      0.41      0.28     32117\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "base_MLP_s MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[ 395  511 2041  889]\n",
      " [ 117 3168 2997 1401]\n",
      " [ 250 1644 6402 2738]\n",
      " [ 107  970 2950 7767]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.45      0.10      0.17      3836\n",
      "    negative       0.50      0.41      0.45      7683\n",
      "     neutral       0.44      0.58      0.50     11034\n",
      "    positive       0.61      0.66      0.63     11794\n",
      "\n",
      "    accuracy                           0.52     34347\n",
      "   macro avg       0.50      0.44      0.44     34347\n",
      "weighted avg       0.51      0.52      0.50     34347\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "top_MLP_e GridSearchCV(estimator=MLPClassifier(max_iter=1),\n",
      "             param_grid={'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
      "                         'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
      "                         'max_iter': [1], 'solver': ['sgd', 'adam']}):{'cv': None, 'error_score': nan, 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__batch_size': 'auto', 'estimator__beta_1': 0.9, 'estimator__beta_2': 0.999, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (100,), 'estimator__learning_rate': 'constant', 'estimator__learning_rate_init': 0.001, 'estimator__max_fun': 15000, 'estimator__max_iter': 1, 'estimator__momentum': 0.9, 'estimator__n_iter_no_change': 10, 'estimator__nesterovs_momentum': True, 'estimator__power_t': 0.5, 'estimator__random_state': None, 'estimator__shuffle': True, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': False, 'estimator__warm_start': False, 'estimator': MLPClassifier(max_iter=1), 'n_jobs': None, 'param_grid': {'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'activation': ['logistic', 'tanh', 'relu', 'identity'], 'solver': ['sgd', 'adam'], 'max_iter': [1]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n",
      "Confusion Matrix: \n",
      "[[ 888   23    3    0    3    0    0    2    0    0    1    1    0   86\n",
      "     8   78  995   17    0    1]\n",
      " [  74  247    9    0    1    0    0    0    0    0    5    1    0   19\n",
      "     9   13  828    1    0    3]\n",
      " [  15   16  107    2    1    1    0    4    0    2   10   12    0    6\n",
      "     0   13  837    4    0   15]\n",
      " [  18   20   47    3    0    1    0    2    1    1   18   13    0   11\n",
      "     1   13 1477    3    0   22]\n",
      " [ 161   22    6    0    5    3    0    1    0    0   19    1    0   30\n",
      "     5   55 1921   13    1    9]\n",
      " [  39    2    2    0    1    9    0    0    0    0    7    0    0   36\n",
      "     7   10  536   17    0    7]\n",
      " [  13   10    5    0    0    0    1    6    0    0   22    3    0    3\n",
      "     0    7  907    5    1   12]\n",
      " [  23    9    7    0    0    3    0   31    0    0    4    2    0   11\n",
      "     0   11 1076    2    0    3]\n",
      " [  21    0    1    0    0    0    0    0    0    0    1    0    0   23\n",
      "     2   25  325   20    0    5]\n",
      " [  19   12    5    0    1    1    0    0    0    3   11    5    0    6\n",
      "     1   12  845    4    0   43]\n",
      " [  20   13    7    0    0    0    0    1    0    0   55    8    0   10\n",
      "     1   10 1333    4    0   10]\n",
      " [  14    7   24    0    0    0    0    0    0    1    9   33    0    1\n",
      "     0    5  498    2    0   11]\n",
      " [   8    4    4    0    0    0    1    1    0    0    1   23    0    0\n",
      "     0    1  280    1    0   23]\n",
      " [  73    7    0    0    0    1    0    0    0    0    1    0    0 1094\n",
      "     6    9  281    6    1    4]\n",
      " [ 129   44    0    0    2    0    0    0    0    0    0    0    0   64\n",
      "    35   53  522    8    0    3]\n",
      " [  89    9    1    0    0    0    0    0    0    0    0    0    0   18\n",
      "     3  520  338    3    0    0]\n",
      " [ 396  110   84    1   11    9    0   34    0    3   73   33    0  159\n",
      "    24  139 9853   37    1   67]\n",
      " [  66    0    1    1    3    0    0    0    0    0    2    0    0   51\n",
      "     2   20  697   58    0    3]\n",
      " [   2    6    1    0    0    2    0    0    0    1    5    0    0   21\n",
      "     1    1  206    4   10   46]\n",
      " [  13    8    5    0    0    0    0    0    0    2    7    7    0   11\n",
      "     3   22  545    9    5   98]]\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.38      0.42      0.40      2106\n",
      "     amusement       0.40      0.20      0.27      1210\n",
      "         anger       0.32      0.10      0.16      1045\n",
      "     annoyance       0.43      0.00      0.00      1651\n",
      "      approval       0.17      0.00      0.00      2252\n",
      "        caring       0.30      0.01      0.03       673\n",
      "     confusion       0.50      0.00      0.00       995\n",
      "     curiosity       0.35      0.03      0.05      1182\n",
      "        desire       0.00      0.00      0.00       423\n",
      "disappointment       0.23      0.00      0.01       968\n",
      "   disapproval       0.20      0.04      0.06      1472\n",
      "       disgust       0.21      0.05      0.09       605\n",
      "          fear       0.00      0.00      0.00       347\n",
      "     gratitude       0.63      0.74      0.68      1483\n",
      "           joy       0.25      0.04      0.07       860\n",
      "          love       0.49      0.53      0.51       981\n",
      "       neutral       0.37      0.89      0.52     11034\n",
      "      optimism       0.25      0.06      0.10       904\n",
      "       remorse       0.50      0.03      0.06       306\n",
      "       sadness       0.22      0.13      0.17       735\n",
      "\n",
      "     micro avg       0.38      0.42      0.40     31232\n",
      "     macro avg       0.31      0.16      0.16     31232\n",
      "  weighted avg       0.34      0.42      0.29     31232\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "top_MLP_s GridSearchCV(estimator=MLPClassifier(max_iter=1),\n",
      "             param_grid={'activation': ['logistic', 'tanh', 'relu', 'identity'],\n",
      "                         'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
      "                         'max_iter': [1], 'solver': ['sgd', 'adam']}):{'cv': None, 'error_score': nan, 'estimator__activation': 'relu', 'estimator__alpha': 0.0001, 'estimator__batch_size': 'auto', 'estimator__beta_1': 0.9, 'estimator__beta_2': 0.999, 'estimator__early_stopping': False, 'estimator__epsilon': 1e-08, 'estimator__hidden_layer_sizes': (100,), 'estimator__learning_rate': 'constant', 'estimator__learning_rate_init': 0.001, 'estimator__max_fun': 15000, 'estimator__max_iter': 1, 'estimator__momentum': 0.9, 'estimator__n_iter_no_change': 10, 'estimator__nesterovs_momentum': True, 'estimator__power_t': 0.5, 'estimator__random_state': None, 'estimator__shuffle': True, 'estimator__solver': 'adam', 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': False, 'estimator__warm_start': False, 'estimator': MLPClassifier(max_iter=1), 'n_jobs': None, 'param_grid': {'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'activation': ['logistic', 'tanh', 'relu', 'identity'], 'solver': ['sgd', 'adam'], 'max_iter': [1]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n",
      "Confusion Matrix: \n",
      "[[ 493  723 1552 1068]\n",
      " [ 153 3999 2044 1487]\n",
      " [ 350 2353 5121 3210]\n",
      " [ 134 1395 2145 8120]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.44      0.13      0.20      3836\n",
      "    negative       0.47      0.52      0.50      7683\n",
      "     neutral       0.47      0.46      0.47     11034\n",
      "    positive       0.58      0.69      0.63     11794\n",
      "\n",
      "    accuracy                           0.52     34347\n",
      "   macro avg       0.49      0.45      0.45     34347\n",
      "weighted avg       0.51      0.52      0.50     34347\n",
      "\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding(path=\"embedding.txt\", size=0, vector=google)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjNE7bRH8Me5",
    "outputId": "677f5b9f-5fa1-4b78-8ac3-bc2aecd6ad9f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now begin 3.8\n",
    "\n",
    "base on our result, we select base_MLP_s and base_MLP_e as the best model\n",
    "\n",
    "we will use conceptnet-numberbatch-17-06-300 and glove-twitter-200 for 3.8"
   ],
   "metadata": {
    "id": "MXi23aZXkpD3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gc.collect() # Try to release the previous model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gikd7ystqdJP",
    "outputId": "d7739f0c-55df-44a2-a549-7adca6a1efc2"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "conceptnet = api.load(\"conceptnet-numberbatch-17-06-300\")"
   ],
   "metadata": {
    "id": "5Ww9Bhpgnunh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_mean_vector_conceptnet(word2vec_model,  words):\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [('/c/en/'+word) for word in words if ('/c/en/'+word) in word2vec_model.wv.vocab.keys()]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model[words], axis=0)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def sent_vectorizer_conceptnet(sent, model):\n",
    "    sent_vec = []\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            w = '/c/en/'+w\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw += 1\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "\n",
    "def data_vectorizer_conceptnet(sents, model):\n",
    "    data_vec = []\n",
    "    for sent in sents:\n",
    "        data_vec.append(sent_vectorizer_conceptnet(sent, model))\n",
    "    return data_vec\n",
    "\n",
    "\n",
    "def get_post_hit_rate__conceptnet(model, set):\n",
    "    hits = [1 for i in set if ('/c/en/'+i) in model.vocab.keys()]\n",
    "    return sum(hits)/len(set)"
   ],
   "metadata": {
    "id": "rx5ef9_nygRk"
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here to start our test"
   ],
   "metadata": {
    "id": "XICocW49rTzs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vector = conceptnet"
   ],
   "metadata": {
    "id": "2fk_PgCbsWH-"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = 'conceptnet.txt'\n",
    "size = 0\n",
    "\n",
    "print(vector.vector_size)\n",
    "datas = load_dataset(DATA_PATH)\n",
    "# clean data\n",
    "data = []\n",
    "for value in datas:\n",
    "    words = word_tokenize(value[0])\n",
    "    if len(get_mean_vector_conceptnet(vector, words)) >= 1:\n",
    "        data.append(value)\n",
    "print(\"total remove:\" + str(len(datas) - len(data)))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUrUPwF7rY5K",
    "outputId": "b40318dd-cc45-4c58-d612-c7c25c4570bb"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n",
      "total remove:3451\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 3.2\n",
    "(X_train, X_test, y_train_e, y_test_e), (_, _, y_train_s, y_test_s) = split_data(data, tokenize=True, size_of_data=size)\n",
    "\n",
    "print(\"The size of the tokens is: \", len(X_train))\n",
    "# Compute the embedding of a Reddit post as the average of the embeddings of its words. If\n",
    "# a word has no embedding in Word2Vec, skip it.\n",
    "sentence_embeddings = get_mean_vector_conceptnet(vector, X_train[0])\n",
    "average = sum(sentence_embeddings) / len(sentence_embeddings)\n",
    "print(average)\n",
    "# 3.3\n",
    "embeddings_train = []\n",
    "for post in X_train:\n",
    "    embedding = get_mean_vector_conceptnet(vector, post)\n",
    "    embeddings_train.append(sum(embedding) / len(embedding) if len(embedding) != 0 else 0)\n",
    "\n",
    "print(embeddings_train[0:100])\n",
    "\n",
    "# 3.4\n",
    "# Compute and display the overall hit rates of the training and test sets\n",
    "train_hit_rates = []\n",
    "for post in X_train[0:100]:\n",
    "    train_hit_rates.append(get_post_hit_rate__conceptnet(vector, post))\n",
    "print(\"The hit rate of the training set is: \", sum(train_hit_rates) / len(train_hit_rates))\n",
    "test_hit_rates = []\n",
    "for post in X_test[0:100]:\n",
    "    test_hit_rates.append(get_post_hit_rate__conceptnet(vector, post))\n",
    "print(\"The hit rate of the test set is: \", sum(test_hit_rates) / len(test_hit_rates))\n",
    "\n",
    "# 3.5\n",
    "\n",
    "X_train = data_vectorizer_conceptnet(X_train, vector)\n",
    "X_test = data_vectorizer_conceptnet(X_test, vector)\n",
    "# X_train = embeddings_train\n",
    "# X_test = embeddings_test\n",
    "# test = 0\n",
    "# for value in X_train:\n",
    "#   if test == 0:\n",
    "#     test = value.shape\n",
    "#   elif value.shape != test:\n",
    "#     print(value.shape)\n",
    "#     break\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(\"\")\n",
    "model = MLPClassifier(max_iter=1)\n",
    "base_MLP_e = train_models(model, X_train, y_train_e)\n",
    "save_performance(base_MLP_e, X_test, y_test_e, \"base_MLP_e\", path=path)\n",
    "base_MLP_s = train_models(model, X_train, y_train_s)\n",
    "save_performance(base_MLP_s, X_test, y_test_s, \"base_MLP_s\", path=path)\n",
    "# print(type(base_MLP_e), type(top_MLP_e), type(base_MLP_s), type(top_MLP_s))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NR_h0nVuPlx",
    "outputId": "29e4426a-9de7-4d5f-b5d3-14d00129d1a4"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The size of the tokens is:  134695\n",
      "0.0027759999786455105\n",
      "[0.0027759999786455105, 0.004597627392649883, 0.0033557272900543466, 0.0015531779600132722, 0.0044381112747153865, 0.0020619722680082003, 0.00481611127769914, 0.0012065642263405607, 0.004190333309622171, 0.004024298268608012, 0.004791916652272145, 0.004086871575309487, 0.0026020416830821584, 0.0037949258562154377, 0.004440216741713811, 0.0037962664864001757, 0.002431458255353694, 0.006882000059680043, 0.004750749942128702, 0.004724740783591794, 0.001648812396354818, 0.005697533220087887, 0.0034296665106133637, 0.004309933328089149, 0.004193566677116299, 0.0035174919717246666, 0.0030113809492572073, 0.005124638830602634, 0.005638833368041863, 0.002532666773937914, 0.004236703699534701, 0.0005683334742692144, 0.00185791682296743, 0.0015594665213332821, 0.004577950109863498, 0.0034617110943023967, 0.00018124986168307564, 0.0029343332268823965, 0.004297666730417404, 0.004645846212194253, 0.0031110000334835302, 0.0023491904968326103, 0.004476933607123404, 0.004659857021542848, 0.0029574166564270855, 0.004987500041315798, 0.005386388803757048, 0.005092288846884306, 0.004983633488870207, 0.0036380392369634745, 0.004399933271003344, 0.005669333535042825, 0.0039016666640721573, 0.004834755456334582, 0.004515422365378375, 0.0013625000951287803, 0.006406833352909113, 0.0029841997489954033, 0.002406299935731416, 0.002726999897956072, 0.002981250049682179, 0.004787765739723303, 0.0029515000229973034, 0.005331111238629092, 0.003913288793000902, 0.002811548939495754, 0.005126190514711197, 0.00470749998422131, 0.004089119057777376, 0.0013168889543885597, 0.002296000100322999, -0.0001831111152569065, 0.002530555643340146, 0.004236362257555205, 0.0025918164739717514, 0.00332290004913375, 0.004373616460482784, 0.004359666653860283, 0.0026784582921148587, 0.0025079999997493965, 0.003533373432631682, 0.0035032499447697775, 0.0010845209960825741, 0.0022829394299848597, 0.004190714160140488, 0.003149019391263816, 0.0055005000870975585, 0.004457916762730747, 0.0025898332831275185, 0.00367837875278686, 0.002699979210932118, 0.003729563978946923, 0.002637066683382727, 0.0031779742342284106, 0.0029275833609669157, 0.004309925943153858, 0.0012551665919212003, 0.004461027657865392, 0.005577073920485418, 0.00411811099925823]\n",
      "The hit rate of the training set is:  0.7088477129353654\n",
      "The hit rate of the test set is:  0.6765153558245209\n",
      "base_MLP_e MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[  568     6     4     0     0     0     0     0    57     2    60  1443\n",
      "      3     0     0]\n",
      " [   24   186     5     0     0     0     2     0    15     0    13  1025\n",
      "      0     0     0]\n",
      " [    7    10    36     1     0     0     0     0     3     0    20   884\n",
      "      1     0     2]\n",
      " [    9    16    19     1     0     0     0     0    11     0     7  1562\n",
      "      0     0     0]\n",
      " [   73     9     0     0     9     0     2     1    35     0    26  2121\n",
      "      0     0     0]\n",
      " [    8     4     0     0     0     1     0     0    17     0     6  1111\n",
      "      0     0     0]\n",
      " [   13     3     5     0     0     0     9     0     1     1    10  1425\n",
      "      0     0     0]\n",
      " [   13     5    24     0     0     0     0     3     1     0     3   538\n",
      "      0     0     0]\n",
      " [   46     5     4     0     0     0     0     0   517     4     7   816\n",
      "      1     1     0]\n",
      " [   40    37     0     0     0     0     0     0    42    31    35   627\n",
      "      0     0     0]\n",
      " [   34     5     2     0     0     0     0     0    16     3   324   579\n",
      "      0     0     0]\n",
      " [  163    67    26     0     7     0    13     4    94     8    91 10309\n",
      "      2     0     1]\n",
      " [   34     3     0     0     0     0     0     0    38     0     4   796\n",
      "      4     0     0]\n",
      " [    2     2     2     0     0     0     0     0    16     0     2   286\n",
      "      0     1     0]\n",
      " [    7     2     2     0     0     0     1     1     9     2     4   703\n",
      "      1     1     4]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  admiration       0.48      0.27      0.34      2143\n",
      "   amusement       0.47      0.15      0.22      1270\n",
      "       anger       0.24      0.04      0.06       964\n",
      "   annoyance       0.25      0.00      0.00      1625\n",
      "    approval       0.53      0.00      0.01      2276\n",
      "   curiosity       1.00      0.00      0.00      1147\n",
      " disapproval       0.27      0.01      0.01      1467\n",
      "     disgust       0.23      0.01      0.01       587\n",
      "   gratitude       0.54      0.37      0.44      1401\n",
      "         joy       0.43      0.04      0.07       812\n",
      "        love       0.50      0.34      0.40       963\n",
      "     neutral       0.34      0.96      0.50     10785\n",
      "    optimism       0.21      0.00      0.01       879\n",
      "     remorse       0.33      0.00      0.01       311\n",
      "     sadness       0.36      0.01      0.01       737\n",
      "\n",
      "   micro avg       0.36      0.44      0.39     27367\n",
      "   macro avg       0.41      0.15      0.14     27367\n",
      "weighted avg       0.40      0.44      0.28     27367\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "base_MLP_s MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[ 138  499 2218  853]\n",
      " [  49 2946 3274 1177]\n",
      " [ 116 1468 6647 2554]\n",
      " [  77  912 3701 7045]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.36      0.04      0.07      3708\n",
      "    negative       0.51      0.40      0.44      7446\n",
      "     neutral       0.42      0.62      0.50     10785\n",
      "    positive       0.61      0.60      0.60     11735\n",
      "\n",
      "    accuracy                           0.50     33674\n",
      "   macro avg       0.47      0.41      0.40     33674\n",
      "weighted avg       0.50      0.50      0.48     33674\n",
      "\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "twitter = api.load(\"glove-twitter-200\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh4RnuwL4aZ_",
    "outputId": "1b94f6fd-2c87-48af-f8d7-5af49c4d0809"
   },
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "vector = twitter"
   ],
   "metadata": {
    "id": "-HiUgmXP4dAE"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = 'twitter.txt'\n",
    "size = 0\n",
    "\n",
    "print(vector.vector_size)\n",
    "datas = load_dataset(DATA_PATH)\n",
    "# clean data\n",
    "data = []\n",
    "for value in datas:\n",
    "    words = word_tokenize(value[0])\n",
    "    if len(get_mean_vector(vector, words)) >= 1:\n",
    "        data.append(value)\n",
    "print(\"total remove:\" + str(len(datas) - len(data)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKL2thLl4ttY",
    "outputId": "8352e4b4-9588-4fa9-94d0-52cd62788bfd"
   },
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "200\n",
      "total remove:891\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 3.2\n",
    "(X_train, X_test, y_train_e, y_test_e), (_, _, y_train_s, y_test_s) = split_data(data, tokenize=True, size_of_data=size)\n",
    "\n",
    "print(\"The size of the tokens is: \", len(X_train))\n",
    "# Compute the embedding of a Reddit post as the average of the embeddings of its words. If\n",
    "# a word has no embedding in Word2Vec, skip it.\n",
    "sentence_embeddings = get_mean_vector(vector, X_train[0])\n",
    "average = sum(sentence_embeddings) / len(sentence_embeddings)\n",
    "print(average)\n",
    "# 3.3\n",
    "embeddings_train = []\n",
    "for post in X_train:\n",
    "    embedding = get_mean_vector(vector, post)\n",
    "    embeddings_train.append(sum(embedding) / len(embedding) if len(embedding) != 0 else 0)\n",
    "\n",
    "print(embeddings_train[0:100])\n",
    "\n",
    "# 3.4\n",
    "# Compute and display the overall hit rates of the training and test sets\n",
    "train_hit_rates = []\n",
    "for post in X_train[0:100]:\n",
    "    train_hit_rates.append(get_post_hit_rate(vector, post))\n",
    "print(\"The hit rate of the training set is: \", sum(train_hit_rates) / len(train_hit_rates))\n",
    "test_hit_rates = []\n",
    "for post in X_test[0:100]:\n",
    "    test_hit_rates.append(get_post_hit_rate(vector, post))\n",
    "print(\"The hit rate of the test set is: \", sum(test_hit_rates) / len(test_hit_rates))\n",
    "\n",
    "# 3.5\n",
    "\n",
    "X_train = data_vectorizer(X_train, vector)\n",
    "X_test = data_vectorizer(X_test, vector)\n",
    "# X_train = embeddings_train\n",
    "# X_test = embeddings_test\n",
    "# test = 0\n",
    "# for value in X_train:\n",
    "#   if test == 0:\n",
    "#     test = value.shape\n",
    "#   elif value.shape != test:\n",
    "#     print(value.shape)\n",
    "#     break\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(\"\")\n",
    "model = MLPClassifier(max_iter=1)\n",
    "base_MLP_e = train_models(model, X_train, y_train_e)\n",
    "save_performance(base_MLP_e, X_test, y_test_e, \"base_MLP_e\", path=path)\n",
    "base_MLP_s = train_models(model, X_train, y_train_s)\n",
    "save_performance(base_MLP_s, X_test, y_test_s, \"base_MLP_s\", path=path)\n",
    "# print(type(base_MLP_e), type(top_MLP_e), type(base_MLP_s), type(top_MLP_s))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e13WvYq4pXE",
    "outputId": "5a69e32c-134c-4f9f-cf54-ab9541d29fb6"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The size of the tokens is:  136743\n",
      "-0.011200135269027668\n",
      "[-0.011200135269027668, -0.0199887799289354, -0.016347183499019594, -0.009659170810336945, -0.027371308133297134, 8.109590504318476e-05, -0.017887009701225906, -0.014627880028565415, -0.025901396111876238, -0.030382947868201882, -0.019449520221096462, -0.013652972058043816, -0.01741443986971717, -0.006451923691201955, -0.014489166623588972, -0.02277974577387795, -0.014183362330659293, -0.03476949436575524, -0.020103628892757117, -0.01152558743255213, -0.009663611768046395, -0.042000598944650844, -0.03115049835469108, -0.019926364669809116, -0.014225139124318957, -0.018480668241390958, -0.014032614911266138, -0.028642209699610247, -0.03616293186852999, -0.019387232013395986, -0.007458210779295768, -0.00347560808993876, -0.014756072229938582, -0.027362000172579427, -0.018786824482958764, -0.02176213920596638, -0.02048862477298826, -0.0168986480939202, -0.021829316731309518, -0.026763648065389133, -0.018884597635769752, -0.04059373590163887, -0.01271840525441803, -0.02692260642461406, -0.011385843072057469, -0.020363443947862835, -0.022134306351654233, -0.015595530548889656, -0.005915851922945876, -0.009589970184215417, -0.01853124638598274, -0.006509672355023212, -0.022007160985376684, -0.011166237350880692, -0.016462013092823326, -0.009508187043829821, -0.016314954543777276, -0.02213692998047918, -0.020535976217943244, -0.007653074807021767, -0.004055204030009918, -0.010234633328946075, -0.015778718514338833, -0.009956162305898034, -0.03302401838969672, -0.0022868735110387205, -0.02638911338930484, -0.018568730854021852, -0.015370301341463347, -0.026574472496286035, -0.02044247925747186, -0.028393731666437817, -0.020177183334017173, -0.011888170826714485, -0.021304677147963958, -0.033451309928204866, -0.020959253278560935, -0.00748382128047524, -0.015906663959322033, -0.013345631452575618, -0.005596964080323233, -0.014820255086233374, -0.014533667867654003, -0.02424838063059724, -0.013318559595500119, -0.016995050119585357, -0.0023584118646067507, -0.025172820888110438, -0.019185055439957067, -0.030003947768127547, -0.021174318172343193, -0.008091248211021593, -0.007704380411159946, -0.002303383155667689, -0.01356014946999494, -0.0020940928140771573, -0.009881228997837752, -0.023188115840312093, -0.020439468436525204, -0.03942002024617977]\n",
      "The hit rate of the training set is:  0.8343340150128975\n",
      "The hit rate of the test set is:  0.8323161398546969\n",
      "base_MLP_e MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[  652     7     8     0     1     4     0     2     0     1     0     1\n",
      "      1     0    82     4    37  1262     2     0     1]\n",
      " [   43   155     6     3     1     0     0     4     0     0     0     2\n",
      "      0     0    12     3    10   943     0     0     1]\n",
      " [    9     6    61    10     0     1     0     8     0     0     1     7\n",
      "      0     0     9     0     0   890     0     0     0]\n",
      " [   18     9    53    11     0     0     0     8     0     3     0     6\n",
      "      0     0    12     1     1  1532     0     0     7]\n",
      " [  103     7     3     4    17     0     1     0     0     0     2     2\n",
      "      0     0    62     0     6  2059     4     0     5]\n",
      " [   12     0     1     0     0     9     0     1     0     0     1     2\n",
      "      0     0    56     3     4   632     8     1     3]\n",
      " [   13     0     2     1     0     0    12    55     0     1     3     0\n",
      "      0     0     3     0     0   886     0     0     1]\n",
      " [    9     4     0     1     0     0    11   148     0     0     0     0\n",
      "      0     0    11     0     1  1028     0     0     2]\n",
      " [   17     4     2     0     0     1     0     0     1     0     0     0\n",
      "      0     0     9     0     6   372    11     0     0]\n",
      " [   12     4     8     1     1     0     0     1     0     5     2     2\n",
      "      0     0    10     1     4   852     1     0    12]\n",
      " [   17     6    11     5     1     0     0     2     0     0    14     4\n",
      "      0     0     6     0     2  1496     0     0     5]\n",
      " [    6     5    14     3     0     0     0     2     0     1     1    40\n",
      "      0     0     1     0     1   483     0     0     3]\n",
      " [   54     6     4     1     0     0     0     3     0     0     0     0\n",
      "      2     0    23    12     5   473     4     0     0]\n",
      " [    7     0     2     0     0     1     0     0     0     0     0    15\n",
      "      0     6     0     0     0   342     1     0     2]\n",
      " [   51     3     5     0     0     1     0     1     0     0     0     0\n",
      "      0     0   594     5     8   715     5     1     0]\n",
      " [   66    27     1     0     0     2     2     1     0     0     0     1\n",
      "      0     0    54    37    25   637     2     0     5]\n",
      " [   59     6     2     0     0     0     0     0     0     0     0     0\n",
      "      0     0    23     0   315   599     0     0     1]\n",
      " [  246    57    51    10     5     6    10   131     0     4    13    23\n",
      "      0     1   123     9    31 10240    16     0    14]\n",
      " [   30     0     2     0     0     1     0     3     1     0     0     0\n",
      "      0     0    74     4     4   732    30     0     2]\n",
      " [    0     1     0     1     0     0     1     1     0     0     0     0\n",
      "      0     0    24     0     2   265     0     1     3]\n",
      " [   10     4     4     3     0     2     0     0     0     2     1     8\n",
      "      0     0    12     1     9   711     1     1    35]]\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    admiration       0.43      0.32      0.37      2065\n",
      "     amusement       0.47      0.13      0.21      1183\n",
      "         anger       0.24      0.06      0.10      1002\n",
      "     annoyance       0.20      0.01      0.01      1661\n",
      "      approval       0.63      0.01      0.01      2275\n",
      "        caring       0.29      0.01      0.02       733\n",
      "     confusion       0.32      0.01      0.02       977\n",
      "     curiosity       0.39      0.12      0.19      1215\n",
      "        desire       0.50      0.00      0.00       423\n",
      "disappointment       0.28      0.01      0.01       916\n",
      "   disapproval       0.34      0.01      0.02      1569\n",
      "       disgust       0.34      0.07      0.12       560\n",
      "    excitement       0.67      0.00      0.01       587\n",
      "          fear       0.86      0.02      0.03       376\n",
      "     gratitude       0.48      0.43      0.45      1389\n",
      "           joy       0.45      0.04      0.08       860\n",
      "          love       0.66      0.31      0.43      1005\n",
      "       neutral       0.35      0.93      0.51     10990\n",
      "      optimism       0.33      0.03      0.06       883\n",
      "       remorse       0.20      0.00      0.01       299\n",
      "       sadness       0.31      0.04      0.08       804\n",
      "\n",
      "     micro avg       0.36      0.39      0.38     31772\n",
      "     macro avg       0.42      0.12      0.13     31772\n",
      "  weighted avg       0.39      0.39      0.26     31772\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "base_MLP_s MLPClassifier(max_iter=1):{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "Confusion Matrix: \n",
      "[[ 805  662 1536  804]\n",
      " [ 161 3416 2685 1431]\n",
      " [ 485 2020 5463 3022]\n",
      " [ 129 1314 2871 7382]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   ambiguous       0.51      0.21      0.30      3807\n",
      "    negative       0.46      0.44      0.45      7693\n",
      "     neutral       0.44      0.50      0.46     10990\n",
      "    positive       0.58      0.63      0.61     11696\n",
      "\n",
      "    accuracy                           0.50     34186\n",
      "   macro avg       0.50      0.45      0.46     34186\n",
      "weighted avg       0.50      0.50      0.49     34186\n",
      "\n",
      "--------------------------------------------\n",
      "\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
